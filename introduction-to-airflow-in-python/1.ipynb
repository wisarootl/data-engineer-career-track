{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- running a workflow in Airflow\n",
    "\n",
    "```shell\n",
    "airflow tasks test <dag_id> <task_id> [execution_date]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "airflow dags list\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the default_args dictionary\n",
    "default_args = {\n",
    "  'owner': 'dsmith',\n",
    "  'start_date': datetime(2023, 1, 14),\n",
    "  'retries': 2\n",
    "}\n",
    "\n",
    "# Instantiate the DAG object\n",
    "with DAG(\"example_etl\", default_args=default_args) as etl_dag:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airflow web UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "airflow webserver -p 9090\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the BashOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "with DAG(dag_id=\"test_dag\", default_args={\"start_date\": \"2024-01-01\"}) as analytics_dag:\n",
    "  # Define the BashOperator \n",
    "  cleanup = BashOperator(\n",
    "      task_id='cleanup_task',\n",
    "      # Define the bash_command\n",
    "      bash_command='cleanup.sh',\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a second operator to run the `consolidate_data.sh` script\n",
    "consolidate = BashOperator(\n",
    "    task_id='consolidate_task',\n",
    "    bash_command='consolidate_data.sh'\n",
    "    )\n",
    "\n",
    "# Define a final operator to execute the `push_data.sh` script\n",
    "push_data = BashOperator(\n",
    "    task_id='pushdata_task',\n",
    "    bash_command='push_data.sh'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airflow tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new pull_sales task\n",
    "pull_sales = BashOperator(\n",
    "    task_id='pullsales_task',\n",
    "    bash_command='wget https://salestracking/latestinfo?json'\n",
    ")\n",
    "\n",
    "# use bitshift to define upstream and downstream\n",
    "# Set pull_sales to run prior to cleanup\n",
    "pull_sales >> cleanup\n",
    "\n",
    "# Configure consolidate to run after cleanup\n",
    "consolidate << cleanup\n",
    "\n",
    "# Set push_data to run last\n",
    "consolidate >> push_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- check dags imported error\n",
    "\n",
    "```shell\n",
    "airflow dags list-import-errors\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_file(URL, savepath):\n",
    "    r = requests.get(URL)\n",
    "    with open(savepath, 'wb') as f:\n",
    "        f.write(r.content)   \n",
    "    # Use the print method for logging\n",
    "    print(f\"File pulled from {URL} and saved to {savepath}\")\n",
    "\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "# Create the task\n",
    "pull_file_task = PythonOperator(\n",
    "    task_id='pull_file',\n",
    "    # Add the callable\n",
    "    python_callable=pull_file,\n",
    "    # Define the arguments\n",
    "    op_kwargs={'URL':'http://dataserver/sales.json', 'savepath':'latestsales.json'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_file_task = PythonOperator(\n",
    "    task_id='parse_file',\n",
    "    # Set the function to call\n",
    "    python_callable=parse_file,\n",
    "    # Add the arguments\n",
    "    op_kwargs={'inputfile':'latestsales.json', 'outputfile':'parsedfile.json'}\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Operator\n",
    "from airflow.operators.email import EmailOperator\n",
    "\n",
    "# Define the task\n",
    "email_manager_task = EmailOperator(\n",
    "    task_id='email_manager',\n",
    "    to='manager@datacamp.com',\n",
    "    subject='Latest sales JSON',\n",
    "    html_content='Attached is the latest sales JSON file as requested.',\n",
    "    files='parsedfile.json',\n",
    "    dag=process_sales_dag\n",
    ")\n",
    "\n",
    "# Set the order of tasks\n",
    "pull_file_task >> parse_file_task >> email_manager_task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airflow Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Set the start date of the DAG to November 1, 2023.\n",
    "- Configure the retry_delay to 20 minutes.\n",
    "- Use the cron syntax to configure a schedule of every Wednesday at 12:30pm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the scheduling arguments as defined\n",
    "default_args = {\n",
    "  'owner': 'Engineering',\n",
    "  'start_date': datetime(2023, 11, 1),\n",
    "  'email': ['airflowresults@datacamp.com'],\n",
    "  'email_on_failure': False,\n",
    "  'email_on_retry': False,\n",
    "  'retries': 3,\n",
    "  'retry_delay': timedelta(minutes=20)\n",
    "}\n",
    "\n",
    "dag = DAG('update_dataflows', default_args=default_args, schedule_interval='30 12 * * 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airflow sensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- check condition until true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.providers.http.operators.http import SimpleHttpOperator\n",
    "from airflow.sensors.filesystem import FileSensor\n",
    "\n",
    "dag = DAG(\n",
    "   dag_id = 'update_state',\n",
    "   default_args={\"start_date\": \"2023-10-01\"}\n",
    ")\n",
    "\n",
    "# check file existing\n",
    "precheck = FileSensor(\n",
    "   task_id='check_for_datafile',\n",
    "   filepath='salesdata_ready.csv',\n",
    "   dag=dag)\n",
    "\n",
    "part1 = BashOperator(\n",
    "   task_id='generate_random_number',\n",
    "   bash_command='echo $RANDOM',\n",
    "   dag=dag\n",
    ")\n",
    "\n",
    "import sys\n",
    "def python_version():\n",
    "   return sys.version\n",
    "\n",
    "part2 = PythonOperator(\n",
    "   task_id='get_python_version',\n",
    "   python_callable=python_version,\n",
    "   dag=dag)\n",
    "   \n",
    "part3 = SimpleHttpOperator(\n",
    "   task_id='query_server_for_external_ip',\n",
    "   endpoint='https://api.ipify.org',\n",
    "   method='GET',\n",
    "   dag=dag)\n",
    "   \n",
    "precheck >> part3 >> part2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airflow executors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check airflow executor in airflow config\n",
    "\n",
    "```shell\n",
    "cat airflow/airflow.cfg | grep \"executor = \"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "airflow info\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.sensors.filesystem import FileSensor\n",
    "from datetime import datetime\n",
    "\n",
    "report_dag = DAG(\n",
    "    dag_id = 'execute_report',\n",
    "    schedule_interval = \"0 0 * * *\"\n",
    ")\n",
    "\n",
    "precheck = FileSensor(\n",
    "    task_id='check_for_datafile',\n",
    "    filepath='salesdata_ready.csv',\n",
    "    start_date=datetime(2024,1,20),\n",
    "    mode='reschedule', # `poke`: sleep while waiting, `reschedule` : parallel while waiting\n",
    "    dag=report_dag\n",
    ")\n",
    "\n",
    "generate_report_task = BashOperator(\n",
    "    task_id='generate_report',\n",
    "    bash_command='generate_report.sh',\n",
    "    start_date=datetime(2024,1,20),\n",
    "    dag=report_dag\n",
    ")\n",
    "\n",
    "precheck >> generate_report_task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging and troubleshooting in Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```shell\n",
    "airflow dags list-import-errors\n",
    "\n",
    "airflow info\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLAs and reporting in Airflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the timedelta object\n",
    "from datetime import timedelta\n",
    "\n",
    "# Create the dictionary entry\n",
    "default_args = {\n",
    "  'start_date': datetime(2024, 1, 20),\n",
    "  'sla': timedelta(minutes=30)\n",
    "}\n",
    "\n",
    "# Add to the DAG\n",
    "test_dag = DAG('test_workflow', default_args=default_args, schedule_interval=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the timedelta object\n",
    "from datetime import timedelta\n",
    "\n",
    "test_dag = DAG('test_workflow', start_date=datetime(2024,1,20), schedule_interval=None)\n",
    "\n",
    "# Create the task with the SLA\n",
    "task1 = BashOperator(task_id='first_task',\n",
    "                     sla=timedelta(hours=3),\n",
    "                     bash_command='initialize_data.sh',\n",
    "                     dag=test_dag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the email task\n",
    "email_report = EmailOperator(\n",
    "        task_id='email_report',\n",
    "        to='airflow@datacamp.com',\n",
    "        subject='Airflow Monthly Report',\n",
    "        html_content=\"\"\"Attached is your monthly workflow report - please refer to it for more detail\"\"\",\n",
    "        files=['monthly_report.pdf'],\n",
    "        dag=report_dag\n",
    ")\n",
    "\n",
    "# Set the email task to run after the report is generated\n",
    "email_report << generate_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.sensors.filesystem import FileSensor\n",
    "from datetime import datetime\n",
    "\n",
    "default_args={\n",
    "    'email': [\"airflowalerts@datacamp.com\", \"airflowadmin@datacamp.com\"],\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': Falses,\n",
    "    'email_on_success': True,\n",
    "}\n",
    "report_dag = DAG(\n",
    "    dag_id = 'execute_report',\n",
    "    schedule_interval = \"0 0 * * *\",\n",
    "    default_args=default_args\n",
    ")\n",
    "\n",
    "precheck = FileSensor(\n",
    "    task_id='check_for_datafile',\n",
    "    filepath='salesdata_ready.csv',\n",
    "    start_date=datetime(2023,2,20),\n",
    "    mode='reschedule',\n",
    "    dag=report_dag)\n",
    "\n",
    "generate_report_task = BashOperator(\n",
    "    task_id='generate_report',\n",
    "    bash_command='generate_report.sh',\n",
    "    start_date=datetime(2023,2,20),\n",
    "    dag=report_dag\n",
    ")\n",
    "\n",
    "precheck >> generate_report_task\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dc-de",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
